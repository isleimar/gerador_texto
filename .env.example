# .env
# Configurações para o LLM local via Ollama
LLM_MODEL="ollama/llama2:13b"
LLM_BASE_URL="http://localhost:11434/v1"
LLM_TEMPERATURE=0.7

# A chave da API não é usada pelo Ollama local, mas é uma boa prática
# para compatibilidade e pode ser útil para outros modelos no futuro.
OPENAI_API_KEY="ollama"